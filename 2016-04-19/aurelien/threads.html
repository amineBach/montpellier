<!DOCTYPE html>
<html>
  <head>
    <title>Montpellier C++</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="stylesheet" type="text/css" href="style-perso.css">
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse

# Tour d'horizon du multi-threading en C++
[Threads, atomics, Boost.Lockfree, ASIO]

.right-column[
.footnote[Par **Aurélien Regat-Barrel**]
]
---
.left-column[
  ## Plan
]
.right-column[
## Qu'est-ce qu'un thread?
## Parallélisme vs Concurrence
## Programmation lock-free
## Pool de threads
## Présentation d'ASIO
]
---
class: center
# &nbsp;
# Quand un programmeur a un problème de performance, il pense aux threads.
--

# Maintenant, il proba deux lèmes.
---
.left-column[
## Qu'est-ce qu'un thread?
]
.right-column[
D'un point de vue technique, un **processus léger** est un ensemble de ressources gérées par l'OS:
- des zones mémoires spécifiques (pile, TLS, contexte...)
- un fil d'exécution enregistré auprès de l'ordonnanceur

Note: un thread n'est pas qu'un fil d'exécution, c'est aussi une pile et donc un espace mémoire dont la taille n'est pas  négligeable.
]
--
.right-column[
### Mais d'un point de vue plus abstrait, c'est quoi un thread?
]
---
.left-column[
## Qu'est-ce qu'un thread?
]
.right-column[
De manière plus génértale, un thread est un **automate fini** dont les changements d'états sont gérés de façon transparente par le système d'exploitation:
- en cours exécution
- en attente d'exécution
- bloqué (E/S, page fault...)
- ...

Quand un thread est bloqué ou interrompu par l'ordonnanceur, son état est sauvegardé pour être remplacé par le contexte d'exécution d'un autre thread prêt à s'exécuter (*context switch*).
]
---
.left-column[
## Les threads en C++11
]
.right-column[

C++11 a introduit le support des threads au niveau du langage.
- Un thread est défini comme *un flot unique de contrôle dans un programme* ($1.10)

Supporter les threads ne s'est pas résumé à ajouter la classe `std::thread`, mais a nécessité de revisiter le modèle mémoire du langage.

Par exemple, l'initialisation (et non pas l'utilisation) des objets statiques est thread-safe en C++11 (garantie nécessaire à la bonne initialisation des mutex).

Il y a aussi tout un tas de considérations très bas niveau qui entrent en jeu...
]

---
.left-column[
## Les threads en C++11
]
.right-column[
Par exemple, quel est le problème avec ce code en C++98?
```cpp
short n1 = 0;
short n2 = 0;

void thread1() {
    while (true) {
        n1 += 1;
    }
}

void thread2() {
    while (true) {
        n2 += 1;
    }
}```]
--
.right-column[
Le jeu d'instruction des processeurs peut obliger le compilateur à manipuler la mémoire selon un granularité supérieure à celle d'un ```short``` (2 bytes).

Conséquence : ```n1``` et ```n2``` sont vus comme les champs haut et bas d'un même mot de 4 bytes => data race!
]
---
.left-column[
## Concurrence et Parallélisme
]
.right-column[
La **concurrence** est l'accès simultané (compétitif) à une même ressource partagée. Elle introduit le risque de collision entre tâches et donc le besoin de les synchroniser.
- Si au moins une tâche modifie une ressource pendant qu'au moins une autre la lit, une corruption (*data race*) est possible.

Le **parallélisme** est l'exécution simultanée de plusieurs tâches qui peuvent être totalement indépendantes les unes des autres.
- Il n'implique pas nécessairement la concurrence (c'est même souhaitable de l'éviter).

L'utilisation directe des threads dans le code tend à coupler et mélanger ces deux notions. Mais elles gagnent à être distinguées.
- [Plain Threads are the GOTO of todays computing](https://www.youtube.com/watch?v=4OCUEgSNIAY)
]

---
.left-column[
## Synchroniser est coûteux
]
.right-column[

Une **section critique** est identifiée par un début (verrouillage) et une fin (libération du verrou).

Elle constitue un goulot d'étranglement par lequel les flux parallèles sont contraints de se "dé-paralléliser".

La [loi d'Amdahl](https://fr.wikipedia.org/wiki/Loi_d%27Amdahl) montre que le gain maximal attendu est inversement proportionnel à la taille du code non parallélisable (sections critiques):
- 5% en SC => x6 avec 8 coeurs
- 10% en SC => x3 avec 4 coeurs (x8 avec 32!)
- 25% en SC => x3 avec 8 coeurs
- 50% en SC => x1.25 avec un 2eme coeur

C'est pourquoi le temps passé dans les sections critiques doit être le plus court possible.
- L'idéal est de ne pas avoir besoin de se synchroniser!
]

---
.left-column[
## Travaux en cours
]
.right-column[
C++11 n'a fait que poser les bases nécessaires à construire des primitives plus évoluées. Le comité travaille activement à introduire de nouvelles abstractions. 

Plus concrètement, la difficulté du multi-threading se trouve dans la concurrence, pas dans le parallélisme. L'idée des nombreux travaux en cours est de tendre vers un style explicitement parallèle (manipulation de tâches) plutôt qu'un style explicitement concurrentiel (manipulation de mutex).

Idéalement chaque tâche devrait travailler sans effet de bords sur une espace mémoire qui lui est propre (style fonctionnel). Le code est alors beaucoup plus simple à paralléliser et à maintenir.

Quelques pistes en cours d'exploration:
- Standardisation de Boost.ASIO
- Algorithmes parallèles dans la STL
- Future composables
- Co-routines
]

---
class: center
# &nbsp;
# Programmation lock-free
## std::atomic
## Boost.LockFree

---
.left-column[
## Mutex vs Lock-free
]
.right-column[
En général, on recourt à un verrou logiciel (mutex) pour éviter la corruption des données partagées.

Mais certaines modification simples (effectuable en une seule instruction machine) peut-être sécurisées via des primitives spéciales du processeur (de type *compare and swap*).
- Un support hardware est nécessaire.

En réalité il y a toujours un verrouillage d'effectué mais au niveau hardware (cache du processeur)
- c'est beaucoup plus rapide qu'un aller-retour dans le noyau de l'OS mais ça reste plus lent qu'une instruction machine classique
]
---
.left-column[
## std::atomic
]
.right-column[

La programmation *lock-free* est très tentante mais elle se révèle encore plus difficile et subtile que d'utiliser correctement les mutex.

```cpp
std::atomic<int> count{0};
MyClass * ptr = nullptr;

MyClass * useMyClass() {
    if (count == 0) {
        ptr = new MyClass;
    }
    ++count:
    return ptr;
}

void releaseMyClass() {
    if (--count == 0) {
        delete ptr;
    }
}```
]
---
.left-column[
## Boost. LockFree
]
.right-column[

Certaines structures de données peuvent être implémentées sans mutex via des primitives atomiques (```stack```, ```queue```, ```set```, ```map```).

D'autres en revanche semblent impossible à implémenter sans verrou global (```std::list```).

**Boost.LockFree** fournit deux types de listes simplement chaînées (```stack```, ```queue```) capables de fonctionner sans verrou. Ces conteneurs sont bien adaptés au cas du producteur/consommateur.

Elle fournit aussi une version encore plus optimisée de liste FIFO qui se passe des primitives atomiques (et ne dépend que des barrières mémoire). Cette version ne fonctionne que si elle est utilisée avec un seul producteur et un seul consommateur (single-reader single-writer)
]
---
.left-column[
## Boost. LockFree
### Usage
]
.right-column[
On spécifie lors de la declaration si l'on souhaite un redimensionnement dynamique ou non
- auquel cas le conteneur peut être bloquant si un redimensionnement est nécessaire

```cpp

```
]
---
## Thread pools

Dans la mesure où un thread est une ressource coûteuse à créer, une première façon d'abstraire leur utilisation est d'architecturer son code pour qu'il manipule des tâches dont l'exécution est déléguée à un objet spécifique (un *task runner*).

Cet objet va en interne créer un groupe de threads constament en attente d'exécuter une nouvelle tâche (des *worker threads*). Le *task runner* s'occupe donc de répartir le travail en fonction de la capacité de la machine (nombre de CPU). L'intérêt majeur est:
- rentabiliser le coût de création d'un thread en le recyclant une fois sont travail terminé
- rendre le code plus simple à lire et écrire via le concept de tâche 

Exemple de bibliothèques:
- Microsoft PPL
- QtConcurrent
- std::async
- Win32 Thread Pool API

---
## Boost Thread Group

Boost ne fournit pas de thread pool à proprement parler.

Boost.ThreadGroup est une classe très simple qui facilite la gestion d'un groupe de threads, mais ne permet pas de leur assigner une liste de tâches à exécuter.

```cpp
boost::thread_group threads;

for (size_t i = 0; i < std::thread::hardware_concurrency(); ++i) {
    threads.create_thread([]{
        // do something
    });
}

threads.join_all();```

En tant que tel, cette classe n'est donc pas très utile. Mais elle se combien bien avec Boost.Asio par exemple (qui au contraire ne s'occupe que de l'ordonancement de tâches à exécuter).

Astuce: donner un identifiant (chaîne de caractères) aux tâches à exécuter qui sert à nommer temporairement le worker thread qui exécute notre tâche. Cela facilite le débogage sur des machines avec beaucoup de processeurs.

---
## Boost Asio

La vocation première de Boost.Asio n'est pas d'être un thread pool mais il est assez facile de l'utiliser à cette fin. Tout comme Boost.ThreagGroup, c'est une lib header only.

Les deux fonctions de base de `service_io` sont:
- `post`: ajouter une tâche à exécuter
- `run` : exécuter les tâches en cours d'attente

Il suffit de créer plusieurs threads en leur donnant à chacun la fonction `service_io::run()` à exécuter pour obtenir un traitement en parallèle des tâches dans la liste.

```cpp
asio::io_service asio_service;

// créer des tâches à exécuter
for (int i = 0; i < 100; ++i) {
	asio_service.post([]{
		// faire quelque chose
	});
}

boost::thread_group threads;
// créer autant de threads que l'on a de CPU
for (size_t i = 0; i < std::thread::hardware_concurrency(); ++i) {
    threads.create_thread([]{
        asio_service.run();
    });
}

// attendre la fin du traitement
threads.join_all();```
---
## Boost.Asio

`service_io::run()` rend la main dès qu'il n'y a plus de tâches en attente d'exécution. Les threads créés terminent alors leur exécution et ne peuvent pas être recyclés ultérieurement pour un autre lot de tâches à traiter.

Pour éviter cela, il suffit de créer une instance de `asio::work`. Cette classe ne fait rien d'autre qu'informer Asio qu'il y a toujours un travail en cours et qu'il ne faut donc pas terminer l'exécution de `run()`.

Le code devient:

```cpp
asio::io_service asio_service;
auto asio_work = std::make_unique_ptr<asio::io_service::work>();

boost::thread_group threads;
// créer autant de threads que l'on a de CPU
for (size_t i = 0; i < std::thread::hardware_concurrency(); ++i) {
    threads.add(std::bind(&asio::service_io::run, asio_service));
}

// A partir d'ici, les threads sont en attente d'un tâche```

```cpp
// 1er lot de tâches à exécuter
for (size_t i : irange(0, 100)) {
    service.post([]{
        std::this_thread::sleep_for(100ms);
    });
}

// participer au traitement
service.poll();

// 2eme lot de tâches à exécuter
for (size_t i : irange(0, 100)) {
	service.post(std::bind(&std::this_thread::sleep_for, 100ms));
}

// attendre la fin du traitement
asio_work.reset();
threads.join_all();```

---
## Parallélisation avec Map/Reduce (Qt)

std::vector<int> numbers;

// calculer la racine carrée de chaque élément

version maison:
  - la race condition est sur l'iterateur, pas le vecteur

```cpp
std::atomic<int> pos{0};

auto process = [&pos]{
	for (;;) {
		const size_t i = pos++;
		if (i &lt; numbers.size()) {
			numbers[i] = sqrt(numbers[i]);
		}
		else {
			break;
		}
	}
};
QtConcurrent::run(process);

thread_group pool;
for (int i = 0; i &lt; thread::hardware_concurrency(); ++i)) {
	pool.create(process);
}
pool.join_all();
```

QtConcurrent::Map
    </textarea>
    <script src="remark.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create();
    </script>
  </body>
</html>
