<!DOCTYPE html>
<html>
  <head>
    <title>Montpellier C++</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="stylesheet" type="text/css" href="style-perso.css">
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse

# Tour d'horizon du multi-threading en C++
[Threads, atomics, Boost.Lockfree, ASIO]

.right-column[
.footnote[Par **Aurélien Regat-Barrel**]
]
---
.left-column[
  ## Plan
]
.right-column[
## Qu'est-ce qu'un thread?
## Parallélisme vs Concurrence
## Programmation lock-free
## Pool de threads
## Présentation d'ASIO
]
---
class: center
# &nbsp;
# Quand un programmeur a un problème de performance, il pense aux threads.
--

# Maintenant, il proba deux lèmes.
---
.left-column[
## Qu'est-ce qu'un thread?
]
.right-column[
D'un point de vue technique, un **processus léger** est un ensemble de ressources gérées par l'OS:
- des zones mémoires spécifiques (pile, TLS, contexte...)
- un fil d'exécution enregistré auprès de l'ordonnanceur

Note: un thread n'est pas qu'un fil d'exécution, c'est aussi une pile et donc un espace mémoire dont la taille n'est pas  négligeable.
]
--
.right-column[
### Mais d'un point de vue plus abstrait, c'est quoi un thread?
]
---
.left-column[
## Qu'est-ce qu'un thread?
]
.right-column[
De manière plus générale, un thread est un **automate fini** dont les changements d'états sont gérés de façon transparente par le système d'exploitation:
- en cours exécution
- en attente d'exécution
- bloqué (E/S, page fault...)
- ...

Quand un thread est bloqué ou interrompu par l'ordonnanceur, son état est sauvegardé pour être remplacé par le contexte d'exécution d'un autre thread prêt à s'exécuter (*context switch*).
]
---
.left-column[
## Les threads en C++11
]
.right-column[

C++11 a introduit le support des threads au niveau du langage.
- Un thread est défini comme *un flot unique de contrôle dans un programme* ($1.10)

Supporter les threads ne s'est pas résumé à ajouter la classe `std::thread`, mais a nécessité de revisiter le modèle mémoire du langage.
]
--
.right-column[
Par exemple, l'initialisation (et non pas l'utilisation) des objets statiques est thread-safe en C++11 (garantie nécessaire à la bonne initialisation des mutex).

Il y a d'autres considérations bas niveau qui entrent en jeu...
]

---
.left-column[
## Les threads en C++11
]
.right-column[
Par exemple, quel est le problème avec ce code en C++98?
```cpp
short n1 = 0;
short n2 = 0;

void thread1() {
    while (true) {
        n1 += 1;
    }
}

void thread2() {
    while (true) {
        n2 += 1;
    }
}```]
--
.right-column[
Le jeu d'instruction des processeurs peut obliger le compilateur à manipuler la mémoire selon un granularité supérieure à celle d'un ```short``` (2 bytes).

Conséquence : ```n1``` et ```n2``` sont manipulés comme les champs haut et bas d'un même mot de 4 bytes => data race!
]
---
.left-column[
## Concurrence et Parallélisme
]
.right-column[
La **concurrence** est l'accès simultané (compétitif) à une même ressource partagée. Elle introduit le risque de collision entre tâches et donc le besoin de les synchroniser.
- Si au moins une tâche modifie une ressource pendant qu'au moins une autre la lit, une corruption (*data race*) est possible.
]
--
.right-column[
Le **parallélisme** est l'exécution simultanée de plusieurs tâches qui peuvent être totalement indépendantes les unes des autres.
- Il n'implique pas nécessairement la concurrence (c'est même souhaitable de l'éviter).
]
---
.left-column[
## Synchroniser est coûteux
]
.right-column[

Une **section critique** est identifiée par un début (verrouillage) et une fin (libération du verrou).

Elle constitue un goulot d'étranglement par lequel les flux parallèles sont contraints de se "dé-paralléliser".

La [loi d'Amdahl](https://fr.wikipedia.org/wiki/Loi_d%27Amdahl) montre que le gain maximal attendu est inversement proportionnel à la taille du code non parallélisable (sections critiques):
- 5% en SC => x6 avec 8 coeurs
- 10% en SC => x3 avec 4 coeurs (x8 avec 32!)
- 25% en SC => x3 avec 8 coeurs
]
--
.right-column[
C'est pourquoi le temps passé dans les sections critiques doit être le plus court possible!
]
---
.left-column[
## Au delà des threads
]
.right-column[
L'utilisation directe des threads dans le code tend à coupler et mélanger la concurrence au parallélisme.

Mais ces deux notions gagnent à être distinguées car **la difficulté du multi-threading se trouve dans la concurrence**, pas le parallélisme!
- [Plain Threads are the GOTO of todays computing](https://www.youtube.com/watch?v=4OCUEgSNIAY)
]
--
.right-column[
Idéalement chaque tâche devrait travailler sans effet de bords sur une espace mémoire qui lui est propre (style fonctionnel).
- Cela peut justifier une recopie des données avec fusion ultérieure des résultats (Map/Reduce).
]
---
.left-column[
## Au delà des threads
### Travaux en cours
]
.right-column[
C++11 n'a fait que poser les bases nécessaires à construire des primitives plus évoluées. Le comité travaille activement à introduire de nouvelles abstractions. 

L'idée des nombreux travaux en cours est de **tendre vers un style explicitement parallèle** (manipulation de tâches) plutôt qu'un style explicitement concurrentiel (manipulation de mutex).
]
--
.right-column[
Quelques pistes en cours d'exploration:
- Algorithmes parallèles dans la STL (C++17)
- Standardisation de Boost.ASIO
- Composition d'objets futurs
- Coroutines
]

---
class: center
# &nbsp;
# Programmation lock-free
## std::atomic, Boost.LockFree

---
.left-column[
## Mutex vs Lock-free
]
.right-column[
En général, on recourt à un verrou logiciel (mutex) pour éviter la corruption des données partagées. En conséquence, le fil d'exécution peut se vior bloqué, en attente d'obtenir le verrou.

Mais certaines modification simples (effectuable en une seule instruction machine) peuvent être sécurisées sans bloquer le thread appelant via des primitives spéciales du processeur (de type *compare and swap*).
- Un support hardware est nécessaire.
]
--
.right-column[
En réalité il y a toujours un verrouillage d'effectué mais au niveau hardware (cache du processeur)
- C'est beaucoup plus rapide qu'un aller-retour dans le noyau de l'OS mais ça reste plus lent qu'une instruction machine classique
]
---
.left-column[
## std::atomic
]
.right-column[

La programmation *lock-free* est très tentante mais elle se révèle encore plus difficile et subtile que l'utiliser correcte des mutex.

```cpp
std::atomic<int> count{0};
MyClass * ptr = nullptr;

MyClass * useMyClass() {
    if (count == 0) {
        ptr = new MyClass;
    }
    ++count:
    return ptr;
}

void releaseMyClass() {
    if (--count == 0) {
        delete ptr;
    }
}```
Où est le problème?
]
---
.left-column[
## Conteneurs lock-free
]
.right-column[

Certaines structures de données peuvent être implémentées de façon à ne pas bloquer l'appelant même dans un contexte multi-thread (```stack```, ```queue```, ```set```, ```map```).
- D'autres en revanche semblent impossible à implémenter sans verrou global (```std::list```).

**Boost.LockFree** fournit deux types de listes (```stack```, ```queue```) qui fonctionnent sans verrou. Elles sont bien adaptées à un usage producteur / consommateur.

Un autre conteneur FIFO de type buffer circulaire (```spsc_queue```) est aussi fournit. C'est une version **wait free** qui n'est utilisable que s'il y a un seul producteur et un seul consommateur (*single-producer single-consumer queue*).
- Son intérêt est de se passer des primitives atomiques et de ne dépendre que des barrières mémoire.
]
---
.left-column[
## Boost. LockFree
### Principe
]
.right-column[
La possibilité de redimensionner dynamiquement ou non un conteneur est spécifiée au niveau du type (paramètre ```boost::lockfree::fixed_sized```).

```cpp
    template<typename T>
    using lockfree_queue = boost::lockfree::stack<T,
        boost::lockfree::fixed_sized<true>>;

    // taille fixe de 10
    lockfree_queue<int> queue{ 10 };```

Si le conteneur est autorisé à se redimensionner dynamiquement, la garantie d'être non bloquant lors de l'insertion d'un élément avec ```push()``` n'est plus garantie.

Sinon, la demande d'insersion est ignorée (```push()``` renvoie ```false```).
- La variante ```bounded_push()``` fonctionne toujours de cette façon
]
---
.left-column[
## Boost. LockFree
Principe
### Exemple
]
.right-column[
```cpp
	boost::lockfree::stack<
	    int,
		boost::lockfree::capacity<10>> queue;
	
	// remplir jusqu'à saturation
	int n = 0;
	while (queue.push(n))
		++n;
	
	std::cout << queue.top() << "\n"; // "0"
	queue.pop();
	
	queue.consume_all([](int n) {
		std::cout << n << "\n";
	});
	
	assert(queue.empty());```
]
---
.left-column[
## Thread pools
]
.right-column[

Dans la mesure où un thread est une ressource coûteuse à créer, une première façon d'abstraire leur utilisation est d'architecturer son code pour qu'il manipule des tâches dont l'exécution est déléguée à un objet spécifique (un *task runner*).

Cet objet va en interne créer un groupe de threads constament en attente d'exécuter une nouvelle tâche (des *worker threads*). Le *task runner* s'occupe donc de répartir le travail en fonction de la capacité de la machine (nombre de CPU). L'intérêt majeur est:
- rentabiliser le coût de création d'un thread en le recyclant une fois sont travail terminé
- rendre le code plus simple à lire et écrire via le concept de tâche 
]
---
.left-column[
## Boost.Thread
###thread_group
]
.right-column[

Boost ne fournit pas de thread pool, mais une simple classe utilitaire ```thread_group``` qui facilite la création et la gestion d'un groupe de threads.

```cpp
boost::thread_group threads;

for (size_t i = 0; i < std::thread::hardware_concurrency(); ++i) {
    threads.create_thread([]{
        // do something
    });
}

threads.join_all();```

Comme elle ne permet pas d'assigner une liste de tâches aux threads, cette classe n'est pas très utile par elle-même.

Mais elle se combien bien avec Boost.Asio qui au contraire ne s'occupe que de l'ordonnancement de tâches.
]
---
.left-column[
## QtConcurrent
]
.right-column[

De son côté **Qt** fournit un ensemble assez riche de services au sein de son module Concurrent.

Ce module fournit une classe ```QThreadPool``` qui sert 

```cpp
boost::thread_group threads;

for (size_t i = 0; i < std::thread::hardware_concurrency(); ++i) {
    threads.create_thread([]{
        // do something
    });
}

threads.join_all();```

Comme elle ne permet pas d'assigner une liste de tâches aux threads, cette classe n'est pas très utile par elle-même.

Mais elle se combien bien avec Boost.Asio qui au contraire ne s'occupe que de l'ordonnancement de tâches.
]
---
## Boost Asio

La vocation première de Boost.Asio n'est pas d'être un thread pool mais il est assez facile de l'utiliser à cette fin. Tout comme Boost.ThreagGroup, c'est une lib header only.

Les deux fonctions de base de `service_io` sont:
- `post`: ajouter une tâche à exécuter
- `run` : exécuter les tâches en cours d'attente

Il suffit de créer plusieurs threads en leur donnant à chacun la fonction `service_io::run()` à exécuter pour obtenir un traitement en parallèle des tâches dans la liste.

```cpp
asio::io_service asio_service;

// créer des tâches à exécuter
for (int i = 0; i < 100; ++i) {
	asio_service.post([]{
		// faire quelque chose
	});
}

boost::thread_group threads;
// créer autant de threads que l'on a de CPU
for (size_t i = 0; i < std::thread::hardware_concurrency(); ++i) {
    threads.create_thread([]{
        asio_service.run();
    });
}

// attendre la fin du traitement
threads.join_all();```
---
## Boost.Asio

`service_io::run()` rend la main dès qu'il n'y a plus de tâches en attente d'exécution. Les threads créés terminent alors leur exécution et ne peuvent pas être recyclés ultérieurement pour un autre lot de tâches à traiter.

Pour éviter cela, il suffit de créer une instance de `asio::work`. Cette classe ne fait rien d'autre qu'informer Asio qu'il y a toujours un travail en cours et qu'il ne faut donc pas terminer l'exécution de `run()`.

Le code devient:

```cpp
asio::io_service asio_service;
auto asio_work = std::make_unique_ptr<asio::io_service::work>();

boost::thread_group threads;
// créer autant de threads que l'on a de CPU
for (size_t i = 0; i < std::thread::hardware_concurrency(); ++i) {
    threads.add(std::bind(&asio::service_io::run, asio_service));
}

// A partir d'ici, les threads sont en attente d'un tâche```

```cpp
// 1er lot de tâches à exécuter
for (size_t i : irange(0, 100)) {
    service.post([]{
        std::this_thread::sleep_for(100ms);
    });
}

// participer au traitement
service.poll();

// 2eme lot de tâches à exécuter
for (size_t i : irange(0, 100)) {
	service.post(std::bind(&std::this_thread::sleep_for, 100ms));
}

// attendre la fin du traitement
asio_work.reset();
threads.join_all();```

---
## Parallélisation avec Map/Reduce (Qt)

std::vector<int> numbers;

// calculer la racine carrée de chaque élément

version maison:
  - la race condition est sur l'iterateur, pas le vecteur

```cpp
std::atomic<int> pos{0};

auto process = [&pos]{
	for (;;) {
		const size_t i = pos++;
		if (i &lt; numbers.size()) {
			numbers[i] = sqrt(numbers[i]);
		}
		else {
			break;
		}
	}
};
QtConcurrent::run(process);

thread_group pool;
for (int i = 0; i &lt; thread::hardware_concurrency(); ++i)) {
	pool.create(process);
}
pool.join_all();
```

QtConcurrent::Map
    </textarea>
    <script src="remark.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create();
    </script>
  </body>
</html>
